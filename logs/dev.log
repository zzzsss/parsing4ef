2016.5.11 <dev-commited> Adding.
2016.5.13 <dev-c> Changing.
2016.5.15 <dev-c> Slightly Changes.
2016.7.7  <dev-c> Add DpDictionary, to be continued.
2016.7.19 <dev-c>(dev1.5) Continue on DpDictionary, maybe ok.
2016.10.01<dev-c>(tmp) Working on ef-components, now focusing on Score&Rank.
2016.10.02<dev-c>(tmp) Working on Agenda::rank_them, next for the beam generation.
2016.10.03<dev-c>(tmp) Next, need to complete Agenda::alter_beam (for updates).
2016.10.04<dev-c>(tmp) Next-step, on Scorer and Model.
2016.10.05<dev-c>(tmp) Next, finish the component dir.
2016.10.06<dev-c>(tmp) Next, finish the outside part.
2016.10.07<dev-c>(dev1.6) Ok, the one with ModelDummy, compile ok.
2016.10.08<dev-c>(dev1.6.2) [fixing-bug] Still got problems (State.is_correct, too slow).
2016.10.09<dev-c>(dev1.6.3) [fixing] Seems ok, (caculate destiny, -O3, memory-of-Model, small changes).
2016.10.09<dev-c>(dev1.6.4) Some small changes, next really need to run through it and RankLoss.
2016.10.10<dev-c>(dev1.6.5) Big change on State and StateTemp, now stabilize everyone.
2016.10.11<dev-c>(dev1.6.6) [move-on] Next on the Model to finish, too lazy to check the current.
2016.10.12<dev-c>(dev1.7) [dynet] Can run with ModelDynet, let us see the bugs.
2016.10.13<dev-c>(dev1.8) [60%+] Can run at 60% acc on dev after first iter with default+beam6, need more test.
2016.10.14:
	- It seems the results of train with gold insertion can reach 90%+, will thta be overfitting?
	- But have to see the results of no-gold insertion for training set and ONCE-AGAIN step-by-step check!!
2016.10.14:(2)
	- The problem seems to be the gold-insertion for training, not overfitting.
	- But again need step-by-step checking ...
	- [BUG-FIX] In FeatureManager::feature_expand, forget checking NON_EXIST.
2016.10.14<dev-c>(dev1.9) [90%+] Can run 90% at dev with early-update (before fixing the NON_EXIST bug).
2016.10.15:
	- Problem: with till_the_end+margin, can not learn at the first steps.(PERCP-update should with un-dropped best).
2016.10.15<dev-c>(dev1.10) [loss here] Seems some part is fine.
	-- Add is_dropped(), see could this help perceptron.
	-- Seems ok with arc-std+strict-recomb+margin+*+perceptron, now testing.
	-- Add save curr model after each iteration.
	-- [Problem]: unkown stop when dev-testing for a-crf loss (NAN problem).
	-- [Fix]: exp too large, need to minus the largest one (safe exp).
2016.10.15<dev-c>(dev1.10.1) Slightly change, add TE-WMU update mode.
2016.10.16:
	-- Add an option of drop_is_drop, to force drop in training.
	-- Current default hyper-params: margin1.0, udiv-cur, beam(2/4/8), recomb-strict, g-inum1, 50/30/30/30, 0.04-0.6-12-cut3-mb2, drop_is_drop:0
	-- safe exp for loss-rank.
	-- add iter-Changable options.
2016.10.16<dev-r>(dev1.11) [RC1] now explore those (efstd may be ok).
2016.10.17<dev-c>(dev1.11.1) Fix a small bug.
	- Now let's see what are the strategies: (when gold drops or at the end) to update, to stop searching, alter beam (keep gold)
	- update-strategy * loss * margin.
2016.10.18<dev-c>(dev1.11.2) RankingLoss includes more functions.
	- Now it can be exp(x^alpha) or x^alpha.
2016.10.19<dev-c>(dev1.11.3) Refine the recom-repr part.
2016.10.21<dev-c>(dev1.11.4) Really need to tune the features a little?? Or need to add more complex nn??
2016.10.23:
	-- next missions: loss(label/structure), feature, lstm
2016.10.24<dev-uc>(pre-dev1.12) [unchecked] add new ones
	- loss(l/s), s-feature, blstm, dev-nocut
2016.10.25<dev-uc>(pre2-dev1.12) [unchecked] Fix the issues (mainly memory issues)
2016.10.26<dev-c>(dev1.12) [maybe] Small fixes.
	- NOPE_repr in lstm, inum bug
2016.10.28<dev-c>(dev1.12.1) Add embed-init and dropout.
2016.10.30<dev-c>(dev1.12.2) Add mloss_span and fix test small bug.
2016.10.31<dev-c>(dev1.12.3) Add combined embed init and some scripts.
